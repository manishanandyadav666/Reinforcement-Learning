{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "expected-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions= actions # a list\n",
    "        self.lr= learning_rate\n",
    "        self.gamma= reward_decay\n",
    "        self.epsilon= e_greedy\n",
    "        self.q_table= pd.DataFrame(columns= self.actions, dtype= np.float64)\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        # action selection\n",
    "        if np.random.uniform()<self.epsilon:\n",
    "            # Choose the random action\n",
    "            action= np.random.choice(self.actions)\n",
    "        else:\n",
    "            # Choose the best action\n",
    "            state_action= self.q_table.loc[observation, :]\n",
    "            #Some actions have same value\n",
    "            state_action= state_action.reindex(np.random.permutation(state_action.index))\n",
    "            action= state_action.idxmax()   \n",
    "        return action\n",
    "    \n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        q_predict= self.q_table.loc[s, a]\n",
    "        \n",
    "        if s!='terminal':\n",
    "            q_target= r+ self.gamma*self.q_table.loc[s_,:].max()\n",
    "        else:\n",
    "            q_target= r\n",
    "            \n",
    "        self.q_table.loc[s,a]+=  self.lr*(q_target- q_predict) #updating q-value\n",
    "        \n",
    "        \n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            self.q_table= self.q_table.append(\n",
    "            pd.Series([0]*len(self.actions),\n",
    "                     index= self.q_table.columns, \n",
    "                     name=state))\n",
    "            \n",
    "        \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "small-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSA_Markov():\n",
    "    def __init__(self,\n",
    "                n_channel,\n",
    "                n_su,\n",
    "                sense_error_prob_max=0.2,\n",
    "                punish_interfer_PU=-2):\n",
    "        self.n_channel= n_channel # The number of channels\n",
    "        self.n_su= n_su # then number of secondary users\n",
    "        \n",
    "        # Initialize the Markov channels\n",
    "        self._build_Markov_channel()\n",
    "        \n",
    "        # Initialize the locations of SUs and PUs\n",
    "        self._build_location()\n",
    "        \n",
    "        # Set the noise(Mw)\n",
    "        self.Noise= 1*np.float_power(10,-8)\n",
    "        # Set the carrier frequency (5 Ghz)\n",
    "        self.fc= 5\n",
    "        # Set the k in channel gain\n",
    "        self.k= 8\n",
    "        # Set the power of PU and SU(mw)\n",
    "        self.SU_power= 20\n",
    "        self.PU_power= 40\n",
    "        \n",
    "        # Initialize SINR(no consideration of interference of SUs)\n",
    "        self.render_SINR()\n",
    "        \n",
    "        self.n_actions= n_channel+1 # The action space size\n",
    "        self.n_features= n_channel # The sensing result space\n",
    "        \n",
    "        self.sense_error_prob_max= sense_error_prob_max\n",
    "        self.sense_error_prob= np.random.uniform(0, self.sense_error_prob_max, size=(self.n_su, self.n_channel))\n",
    "        \n",
    "        # The punishment for interferring PUs\n",
    "        self.punish_interfer_PU= punish_interfer_PU\n",
    "        \n",
    "    def _build_Markov_channel(self):\n",
    "        # Initialize channel state(uniform distribution)\n",
    "        # 1: Inactive Pu\n",
    "        # 0: Active Pu\n",
    "        \n",
    "        self.channel_state= np.random.choice(2, self.n_channel)\n",
    "        \n",
    "        # Initialize the transition probability of independent channels\n",
    "        self.stayGood_prob= np.random.uniform(0.7, 1, self.n_channel)\n",
    "        self.stayBad_prob= np.random.uniform(0, 0.3, self.n_channel)\n",
    "        self.goodToBad_prob= 1- self.stayGood_prob\n",
    "        self.badToGood_prob= 1- self.stayBad_prob\n",
    "        \n",
    "    def _build_location(self):\n",
    "        # Initialize the location of PUs\n",
    "        self.PU_TX_x= np.random.uniform(0, 150, self.n_channel)\n",
    "        self.PU_TX_y= np.random.uniform(0, 150, self.n_channel)\n",
    "        self.PU_RX_x= np.random.uniform(0, 150, self.n_channel)\n",
    "        self.PU_RX_y= np.random.uniform(0, 150, self.n_channel)\n",
    "        \n",
    "        \n",
    "        # Initialize the location of SUs transmitters\n",
    "        self.SU_TX_x= np.random.uniform(0+40, 150-40, self.n_su)\n",
    "        self.SU_TX_y= np.random.uniform(0+40, 150-40, self.n_su)\n",
    "        \n",
    "        # Initialize the distance between SU's transmitter and receiver\n",
    "        self.SU_d= np.random.uniform(20, 40, self.n_su)\n",
    "        \n",
    "        # Initialize the location of SUs receivers\n",
    "        SU_theda= 2*np.pi*np.random.uniform(0, 1, self.n_su)\n",
    "        SU_dx= self.SU_d*np.cos(SU_theda)\n",
    "        SU_dy= self.SU_d*np.sin(SU_theda)\n",
    "        self.SU_RX_x= self.SU_TX_x+ SU_dx\n",
    "        self.SU_RX_y= self.SU_TX_y+ SU_dy\n",
    "        \n",
    "        # Compute the distance between PU_Tx and SU_Rx\n",
    "        self.SU_RX_PU_TX_d= np.zeros((self.n_su, self.n_channel))\n",
    "        for k in range(self.n_su):\n",
    "            for l in range(self.n_channel):\n",
    "                self.SU_RX_PU_TX_d[k][l]= np.sqrt(\n",
    "                np.float_power((self.SU_RX_x[k]-self.PU_TX_x[l]),2)+\n",
    "                    np.float_power((self.SU_RX_y[k]-self.PU_TX_y[l]), 2)\n",
    "                )\n",
    "        \n",
    "        # Compute the distance between SU_TX and SU_RX\n",
    "        self.SU_RX_SU_TX_d= np.zeros((self.n_su, self.n_su))\n",
    "        for k1 in range(self.n_su):\n",
    "            for k2 in range(self.n_su):\n",
    "                self.SU_RX_SU_TX_d[k1][k2]= np.sqrt(\n",
    "                np.float_power(self.SU_RX_x[k1]-self.SU_TX_x[k2], 2)+\n",
    "                    np.float_power(self.SU_RX_y[k1]-self.SU_TX_y[k2], 2)\n",
    "                )\n",
    "        # Plot the locations\n",
    "        plt.plot(self.PU_TX_x, self.PU_TX_y,'ro',label='PU_TX' )\n",
    "        plt.plot(self.PU_RX_x, self.PU_RX_y,'rx' ,label='PU_RX')\n",
    "        plt.plot(self.SU_TX_x, self.SU_TX_y,'bs',label='SU_TX')\n",
    "        plt.plot(self.SU_RX_x, self.SU_RX_y,'b^',label='SU_RX')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.ylabel('y')\n",
    "        plt.xlabel('x')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def store_action(self, action):\n",
    "        self.action= action\n",
    "        \n",
    "    def sense(self):\n",
    "        tmp_dice= np.random.uniform(0, 1, size=(self.n_su, self.n_channel))\n",
    "        error_index= tmp_dice< self.sense_error_prob # True: sensing error happens, False: sensing is correct\n",
    "        \n",
    "        # Get the sensing result\n",
    "        self.sensing_result= self.channel_state*(1-error_index)+ (1-self.channel_state)*error_index\n",
    "        return self.sensing_result\n",
    "\n",
    "    def access(self, action):\n",
    "        # action[0 - n_channel-1]: \n",
    "        # action n_channel:\n",
    "        self.success=0\n",
    "        self.fail_PU=0\n",
    "        self.fail_collision=0\n",
    "        \n",
    "        self.reward= np.zeros(self.n_su)\n",
    "        \n",
    "            # To check the number of same actions\n",
    "        def eqlen(a, b):\n",
    "            i=0\n",
    "            for j in range(len(a)):\n",
    "                if a[j]==b:\n",
    "                    i=i+1\n",
    "            return i\n",
    "        #Calculate the interference of SUs\n",
    "        Interference_SU=0\n",
    "        SU_sigma2= np.float_power(10, -((41+22.7*np.log10(self.SU_RX_SU_TX_d)+20*np.log10(self.fc/5))/10))\n",
    "        \n",
    "        for k in range(self.n_su):\n",
    "            SU_sigma2[k][k]=0\n",
    "            \n",
    "        for k in range(self.n_su):\n",
    "            if (action[k]==self.n_channel): # Not choosing any channel\n",
    "                self.reward[k]=0\n",
    "            else: \n",
    "                for q in range(self.n_su):\n",
    "                    if (action[q]==action[k]):\n",
    "                        Interference_SU= Interference_SU+ SU_sigma2[k][q]*self.SU_power\n",
    "                SINR= self.H2[k, action[k]]*self.PU_power/(Interference_SU+self.Interference_PU[k, action[k]]+self.Noise)\n",
    "                self.reward[k]= np.log2(1+SINR)\n",
    "                \n",
    "                if (self.channel_state[action[k]]==1):\n",
    "                    #if len(np.where(np.equal(action, action[k]))[0])==1:\n",
    "                    \n",
    "                    if eqlen(action, action[k])==1:\n",
    "                        #Successful transmission\n",
    "                        self.success=self.success+1\n",
    "                    else:\n",
    "                        #Collision with SU\n",
    "                        self.fail_collision=self.fail_collision+1\n",
    "                        \n",
    "                else:\n",
    "                    #collision with PU\n",
    "                    self.fail_PU=self.fail_PU+1\n",
    "                    self.reward[k]= self.punish_interfer_PU\n",
    "                    #if len(np.where(np.equal(action, action[k]))[0])> 1:\n",
    "                    if eqlen(action, action[k])>1:\n",
    "                        #Collision with Su\n",
    "                        self.fail_collision=self.fail_collision+1\n",
    "        return self.reward\n",
    "                \n",
    "                \n",
    "    def render(self):\n",
    "        # The probability of staying in current state in next time-slot\n",
    "        stay_prob= self.channel_state*self.stayGood_prob + (1-self.channel_state)*self.stayBad_prob\n",
    "        \n",
    "        \n",
    "    def render_SINR(self):\n",
    "        # Update the SINR\n",
    "        \n",
    "        # Calculate the channel gain\n",
    "        SU_d= copy.deepcopy(np.reshape(self.SU_d, (-1, 1)))\n",
    "        for n in range(self.n_channel-1):\n",
    "            SU_d= np.hstack((SU_d, np.reshape(self.SU_d, (-1, 1))))\n",
    "            \n",
    "        SU_sigma2= np.float_power(10, -((41+22.7*np.log10(SU_d)+ 20*np.log10(self.fc/5))/10))\n",
    "        CN_real= np.random.normal(0, 1, size=(self.n_su, self.n_channel))\n",
    "        CN_imag= np.random.normal(0, 1, size=(self.n_su, self.n_channel))\n",
    "        theda= np.random.uniform(0, 1, size=(self.n_su, self.n_channel))\n",
    "        H= np.sqrt(self.k/(self.k+1)*SU_sigma2)*np.exp(1j*2*np.pi*theda)+np.sqrt(1/(self.k+1)*SU_sigma2/2)*(CN_real+1j*CN_imag)\n",
    "        self.H2= np.float_power(np.absolute(H), 2)\n",
    "        \n",
    "        # Calculate the interference of PUs\n",
    "        PU_sigma2= np.float_power(10, -((41+22.7*np.log10(self.SU_RX_PU_TX_d)+ 20*np.log10(self.fc/5))/10))\n",
    "        channel_state= np.array([self.channel_state for k in range(self.n_su) ])\n",
    "        self.Interference_PU= self.PU_power*PU_sigma2*(1-channel_state)\n",
    "        \n",
    "        self.SINR= self.H2*self.SU_power/(self.Interference_PU+self.Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "recognized-animation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time= 2000; success=1428; fail_PU=361; fail_collision=133\n",
      "overall_reward_3=4.8436\n",
      "Training time= 4000; success=1420; fail_PU=358; fail_collision=265\n",
      "overall_reward_3=4.8041\n",
      "Training time= 6000; success=1453; fail_PU=327; fail_collision=413\n",
      "overall_reward_3=4.9747\n",
      "Training time= 8000; success=1448; fail_PU=327; fail_collision=566\n",
      "overall_reward_3=4.9399\n",
      "Training time= 10000; success=1434; fail_PU=338; fail_collision=721\n",
      "overall_reward_3=4.8817\n",
      "Training time= 12000; success=1413; fail_PU=357; fail_collision=878\n",
      "overall_reward_3=4.8029\n",
      "Training time= 14000; success=1451; fail_PU=348; fail_collision=1012\n",
      "overall_reward_3=4.9400\n",
      "Training time= 16000; success=1433; fail_PU=360; fail_collision=1153\n",
      "overall_reward_3=4.8788\n",
      "Training time= 18000; success=1448; fail_PU=324; fail_collision=1296\n",
      "overall_reward_3=4.9443\n",
      "Training time= 20000; success=1426; fail_PU=343; fail_collision=1454\n",
      "overall_reward_3=4.8406\n",
      "Training time= 22000; success=1439; fail_PU=347; fail_collision=1604\n",
      "overall_reward_3=4.8855\n",
      "Training time= 24000; success=1437; fail_PU=337; fail_collision=1760\n",
      "overall_reward_3=4.8930\n",
      "Training time= 26000; success=1453; fail_PU=339; fail_collision=1894\n",
      "overall_reward_3=4.9370\n",
      "Training time= 28000; success=1424; fail_PU=353; fail_collision=2051\n",
      "overall_reward_3=4.8235\n",
      "Training time= 30000; success=1447; fail_PU=343; fail_collision=2189\n",
      "overall_reward_3=4.9217\n",
      "Training time= 32000; success=1451; fail_PU=348; fail_collision=2329\n",
      "overall_reward_3=4.9318\n",
      "Training time= 34000; success=1446; fail_PU=342; fail_collision=2456\n",
      "overall_reward_3=4.9242\n",
      "Training time= 36000; success=1412; fail_PU=357; fail_collision=2614\n",
      "overall_reward_3=4.7714\n",
      "Training time= 38000; success=1447; fail_PU=344; fail_collision=2765\n",
      "overall_reward_3=4.9287\n",
      "Epsilon updated to 0.3\n",
      "Training time= 40000; success=1426; fail_PU=356; fail_collision=2921\n",
      "overall_reward_3=4.8434\n",
      "Training time= 42000; success=1431; fail_PU=357; fail_collision=3060\n",
      "overall_reward_3=4.8493\n",
      "Training time= 44000; success=1456; fail_PU=336; fail_collision=3203\n",
      "overall_reward_3=4.9609\n",
      "Training time= 46000; success=1447; fail_PU=360; fail_collision=3331\n",
      "overall_reward_3=4.9190\n",
      "Training time= 48000; success=1418; fail_PU=353; fail_collision=3499\n",
      "overall_reward_3=4.8224\n",
      "Training time= 50000; success=1415; fail_PU=359; fail_collision=3636\n",
      "overall_reward_3=4.8069\n",
      "Training time= 52000; success=1472; fail_PU=341; fail_collision=3752\n",
      "overall_reward_3=5.0216\n",
      "Training time= 54000; success=1424; fail_PU=337; fail_collision=3906\n",
      "overall_reward_3=4.8483\n",
      "Training time= 56000; success=1455; fail_PU=335; fail_collision=4047\n",
      "overall_reward_3=4.9617\n",
      "Training time= 58000; success=1460; fail_PU=351; fail_collision=4172\n",
      "overall_reward_3=4.9592\n",
      "Training time= 60000; success=1430; fail_PU=366; fail_collision=4311\n",
      "overall_reward_3=4.8398\n",
      "Training time= 62000; success=1468; fail_PU=314; fail_collision=4449\n",
      "overall_reward_3=5.0169\n",
      "Training time= 64000; success=1446; fail_PU=349; fail_collision=4587\n",
      "overall_reward_3=4.9159\n",
      "Training time= 66000; success=1425; fail_PU=347; fail_collision=4743\n",
      "overall_reward_3=4.8256\n",
      "Training time= 68000; success=1444; fail_PU=351; fail_collision=4874\n",
      "overall_reward_3=4.8930\n",
      "Training time= 70000; success=1460; fail_PU=325; fail_collision=5007\n",
      "overall_reward_3=4.9802\n",
      "Training time= 72000; success=1440; fail_PU=357; fail_collision=5138\n",
      "overall_reward_3=4.8944\n",
      "Training time= 74000; success=1462; fail_PU=330; fail_collision=5277\n",
      "overall_reward_3=4.9870\n",
      "Training time= 76000; success=1454; fail_PU=333; fail_collision=5417\n",
      "overall_reward_3=4.9670\n",
      "Training time= 78000; success=1447; fail_PU=363; fail_collision=5553\n",
      "overall_reward_3=4.9020\n",
      "Epsilon updated to 0.3\n",
      "Training time= 80000; success=1401; fail_PU=366; fail_collision=5709\n",
      "overall_reward_3=4.7347\n",
      "Training time= 82000; success=1430; fail_PU=363; fail_collision=5852\n",
      "overall_reward_3=4.8363\n",
      "Training time= 84000; success=1420; fail_PU=357; fail_collision=5995\n",
      "overall_reward_3=4.8041\n",
      "Training time= 86000; success=1450; fail_PU=352; fail_collision=6123\n",
      "overall_reward_3=4.9411\n",
      "Training time= 88000; success=1423; fail_PU=342; fail_collision=6276\n",
      "overall_reward_3=4.8487\n",
      "Training time= 90000; success=1455; fail_PU=331; fail_collision=6429\n",
      "overall_reward_3=4.9523\n",
      "Training time= 92000; success=1444; fail_PU=334; fail_collision=6585\n",
      "overall_reward_3=4.9386\n",
      "Training time= 94000; success=1414; fail_PU=358; fail_collision=6748\n",
      "overall_reward_3=4.7956\n",
      "Training time= 96000; success=1474; fail_PU=334; fail_collision=6861\n",
      "overall_reward_3=5.0069\n",
      "Training time= 98000; success=1456; fail_PU=341; fail_collision=6992\n",
      "overall_reward_3=4.9587\n",
      "Training time= 100000; success=1456; fail_PU=349; fail_collision=7112\n",
      "overall_reward_3=4.9427\n",
      "Training time= 102000; success=1434; fail_PU=344; fail_collision=7268\n",
      "overall_reward_3=4.8758\n",
      "Training time= 104000; success=1441; fail_PU=354; fail_collision=7393\n",
      "overall_reward_3=4.9032\n",
      "Training time= 106000; success=1456; fail_PU=329; fail_collision=7521\n",
      "overall_reward_3=4.9741\n",
      "Training time= 108000; success=1429; fail_PU=341; fail_collision=7680\n",
      "overall_reward_3=4.8784\n",
      "Training time= 110000; success=1444; fail_PU=355; fail_collision=7806\n",
      "overall_reward_3=4.9098\n",
      "Training time= 112000; success=1446; fail_PU=335; fail_collision=7951\n",
      "overall_reward_3=4.9310\n",
      "Training time= 114000; success=1432; fail_PU=347; fail_collision=8099\n",
      "overall_reward_3=4.8566\n",
      "Training time= 116000; success=1416; fail_PU=348; fail_collision=8262\n",
      "overall_reward_3=4.8115\n",
      "Training time= 118000; success=1429; fail_PU=339; fail_collision=8423\n",
      "overall_reward_3=4.8472\n",
      "Epsilon updated to 0.3\n",
      "Training time= 120000; success=1447; fail_PU=345; fail_collision=8553\n",
      "overall_reward_3=4.9176\n",
      "Training time= 122000; success=1431; fail_PU=352; fail_collision=8702\n",
      "overall_reward_3=4.8451\n",
      "Training time= 124000; success=1439; fail_PU=340; fail_collision=8844\n",
      "overall_reward_3=4.8912\n",
      "Training time= 126000; success=1434; fail_PU=340; fail_collision=9001\n",
      "overall_reward_3=4.8880\n",
      "Training time= 128000; success=1464; fail_PU=345; fail_collision=9116\n",
      "overall_reward_3=5.0076\n",
      "Training time= 130000; success=1450; fail_PU=344; fail_collision=9247\n",
      "overall_reward_3=4.9363\n",
      "Training time= 132000; success=1445; fail_PU=340; fail_collision=9391\n",
      "overall_reward_3=4.9259\n",
      "Training time= 134000; success=1432; fail_PU=336; fail_collision=9537\n",
      "overall_reward_3=4.8687\n",
      "Training time= 136000; success=1429; fail_PU=350; fail_collision=9680\n",
      "overall_reward_3=4.8552\n",
      "Training time= 138000; success=1455; fail_PU=339; fail_collision=9826\n",
      "overall_reward_3=4.9548\n",
      "Training time= 140000; success=1457; fail_PU=332; fail_collision=9952\n",
      "overall_reward_3=4.9814\n",
      "Training time= 142000; success=1421; fail_PU=342; fail_collision=10127\n",
      "overall_reward_3=4.8313\n",
      "Training time= 144000; success=1422; fail_PU=351; fail_collision=10289\n",
      "overall_reward_3=4.8300\n",
      "Training time= 146000; success=1451; fail_PU=346; fail_collision=10439\n",
      "overall_reward_3=4.9492\n",
      "Training time= 148000; success=1439; fail_PU=333; fail_collision=10595\n",
      "overall_reward_3=4.9086\n",
      "Training time= 150000; success=1452; fail_PU=354; fail_collision=10702\n",
      "overall_reward_3=4.9259\n",
      "Training time= 152000; success=1440; fail_PU=337; fail_collision=10852\n",
      "overall_reward_3=4.9194\n",
      "Training time= 154000; success=1441; fail_PU=348; fail_collision=10995\n",
      "overall_reward_3=4.8986\n",
      "Training time= 156000; success=1445; fail_PU=340; fail_collision=11135\n",
      "overall_reward_3=4.9119\n",
      "Training time= 158000; success=1450; fail_PU=334; fail_collision=11278\n",
      "overall_reward_3=4.9413\n",
      "Epsilon updated to 0.3\n",
      "Training time= 160000; success=1426; fail_PU=348; fail_collision=11425\n",
      "overall_reward_3=4.8438\n",
      "Training time= 162000; success=1458; fail_PU=321; fail_collision=11572\n",
      "overall_reward_3=5.0008\n",
      "Training time= 164000; success=1445; fail_PU=359; fail_collision=11698\n",
      "overall_reward_3=4.8993\n",
      "Training time= 166000; success=1442; fail_PU=342; fail_collision=11850\n",
      "overall_reward_3=4.9152\n",
      "Training time= 168000; success=1441; fail_PU=333; fail_collision=11999\n",
      "overall_reward_3=4.9116\n",
      "Training time= 170000; success=1447; fail_PU=350; fail_collision=12133\n",
      "overall_reward_3=4.9162\n",
      "Training time= 172000; success=1439; fail_PU=346; fail_collision=12277\n",
      "overall_reward_3=4.8822\n",
      "Training time= 174000; success=1441; fail_PU=351; fail_collision=12419\n",
      "overall_reward_3=4.8955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time= 176000; success=1438; fail_PU=335; fail_collision=12579\n",
      "overall_reward_3=4.8944\n",
      "Training time= 178000; success=1429; fail_PU=355; fail_collision=12718\n",
      "overall_reward_3=4.8379\n",
      "Training time= 180000; success=1448; fail_PU=352; fail_collision=12842\n",
      "overall_reward_3=4.9193\n",
      "Training time= 182000; success=1446; fail_PU=345; fail_collision=12972\n",
      "overall_reward_3=4.9289\n",
      "Training time= 184000; success=1456; fail_PU=323; fail_collision=13120\n",
      "overall_reward_3=4.9835\n",
      "Training time= 186000; success=1425; fail_PU=359; fail_collision=13261\n",
      "overall_reward_3=4.8303\n",
      "Training time= 188000; success=1457; fail_PU=341; fail_collision=13396\n",
      "overall_reward_3=4.9685\n",
      "Training time= 190000; success=1475; fail_PU=340; fail_collision=13510\n",
      "overall_reward_3=5.0299\n",
      "Training time= 192000; success=1474; fail_PU=337; fail_collision=13621\n",
      "overall_reward_3=5.0289\n",
      "Training time= 194000; success=1437; fail_PU=345; fail_collision=13774\n",
      "overall_reward_3=4.8874\n",
      "Training time= 196000; success=1459; fail_PU=342; fail_collision=13916\n",
      "overall_reward_3=4.9747\n",
      "Training time= 198000; success=1445; fail_PU=331; fail_collision=14059\n",
      "overall_reward_3=4.9344\n",
      "Epsilon updated to 0.3\n",
      "Training time= 200000; success=1455; fail_PU=351; fail_collision=14198\n",
      "overall_reward_3=4.9485\n",
      "Training time= 202000; success=1454; fail_PU=347; fail_collision=14324\n",
      "overall_reward_3=4.9443\n",
      "Training time= 204000; success=1427; fail_PU=340; fail_collision=14481\n",
      "overall_reward_3=4.8655\n",
      "Training time= 206000; success=1440; fail_PU=344; fail_collision=14622\n",
      "overall_reward_3=4.9011\n",
      "Training time= 208000; success=1424; fail_PU=368; fail_collision=14760\n",
      "overall_reward_3=4.8061\n",
      "Training time= 210000; success=1454; fail_PU=343; fail_collision=14904\n",
      "overall_reward_3=4.9600\n",
      "Training time= 212000; success=1433; fail_PU=356; fail_collision=15045\n",
      "overall_reward_3=4.8708\n",
      "Training time= 214000; success=1448; fail_PU=353; fail_collision=15170\n",
      "overall_reward_3=4.9264\n",
      "Training time= 216000; success=1436; fail_PU=348; fail_collision=15313\n",
      "overall_reward_3=4.9006\n",
      "Training time= 218000; success=1457; fail_PU=347; fail_collision=15440\n",
      "overall_reward_3=4.9387\n",
      "Training time= 220000; success=1455; fail_PU=331; fail_collision=15572\n",
      "overall_reward_3=4.9588\n",
      "Training time= 222000; success=1468; fail_PU=328; fail_collision=15710\n",
      "overall_reward_3=5.0144\n",
      "Training time= 224000; success=1444; fail_PU=328; fail_collision=15874\n",
      "overall_reward_3=4.9394\n",
      "Training time= 226000; success=1451; fail_PU=337; fail_collision=16014\n",
      "overall_reward_3=4.9430\n",
      "Training time= 228000; success=1448; fail_PU=344; fail_collision=16146\n",
      "overall_reward_3=4.9264\n",
      "Training time= 230000; success=1461; fail_PU=325; fail_collision=16292\n",
      "overall_reward_3=4.9978\n",
      "Training time= 232000; success=1446; fail_PU=337; fail_collision=16433\n",
      "overall_reward_3=4.9209\n",
      "Training time= 234000; success=1422; fail_PU=351; fail_collision=16586\n",
      "overall_reward_3=4.8343\n",
      "Training time= 236000; success=1442; fail_PU=349; fail_collision=16723\n",
      "overall_reward_3=4.9016\n",
      "Training time= 238000; success=1448; fail_PU=330; fail_collision=16869\n",
      "overall_reward_3=4.9451\n",
      "Epsilon updated to 0.3\n",
      "Training time= 240000; success=1425; fail_PU=350; fail_collision=17017\n",
      "overall_reward_3=4.8368\n",
      "Training time= 242000; success=1467; fail_PU=329; fail_collision=17127\n",
      "overall_reward_3=5.0153\n",
      "Training time= 244000; success=1439; fail_PU=361; fail_collision=17255\n",
      "overall_reward_3=4.8770\n",
      "Training time= 246000; success=1460; fail_PU=334; fail_collision=17381\n",
      "overall_reward_3=4.9891\n",
      "Training time= 248000; success=1454; fail_PU=345; fail_collision=17516\n",
      "overall_reward_3=4.9514\n",
      "Training time= 250000; success=1434; fail_PU=339; fail_collision=17671\n",
      "overall_reward_3=4.8919\n",
      "Training time= 252000; success=1432; fail_PU=341; fail_collision=17813\n",
      "overall_reward_3=4.8734\n",
      "Training time= 254000; success=1449; fail_PU=324; fail_collision=17971\n",
      "overall_reward_3=4.9444\n",
      "Training time= 256000; success=1439; fail_PU=336; fail_collision=18139\n",
      "overall_reward_3=4.9031\n",
      "Training time= 258000; success=1438; fail_PU=336; fail_collision=18292\n",
      "overall_reward_3=4.8982\n",
      "Training time= 260000; success=1447; fail_PU=355; fail_collision=18432\n",
      "overall_reward_3=4.9342\n",
      "Training time= 262000; success=1463; fail_PU=331; fail_collision=18559\n",
      "overall_reward_3=4.9874\n",
      "Training time= 264000; success=1459; fail_PU=326; fail_collision=18703\n",
      "overall_reward_3=4.9709\n",
      "Training time= 266000; success=1440; fail_PU=344; fail_collision=18852\n",
      "overall_reward_3=4.8804\n",
      "Training time= 268000; success=1428; fail_PU=333; fail_collision=19016\n",
      "overall_reward_3=4.8702\n",
      "Training time= 270000; success=1452; fail_PU=338; fail_collision=19150\n",
      "overall_reward_3=4.9421\n",
      "Training time= 272000; success=1449; fail_PU=346; fail_collision=19284\n",
      "overall_reward_3=4.9284\n",
      "Training time= 274000; success=1441; fail_PU=349; fail_collision=19429\n",
      "overall_reward_3=4.9010\n",
      "Training time= 276000; success=1448; fail_PU=350; fail_collision=19564\n",
      "overall_reward_3=4.9272\n",
      "Training time= 278000; success=1446; fail_PU=349; fail_collision=19691\n",
      "overall_reward_3=4.9173\n",
      "Epsilon updated to 0.3\n",
      "Training time= 280000; success=1450; fail_PU=354; fail_collision=19806\n",
      "overall_reward_3=4.9254\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    random_seed= 3\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Initialize the environment\n",
    "    n_channel=6\n",
    "    n_su=2\n",
    "    env=DSA_Markov(n_channel, n_su)\n",
    "    env_copy= copy.deepcopy(env)\n",
    "    \n",
    "    # Training parameters\n",
    "    if n_channel==6:\n",
    "        batch_size=2000\n",
    "    elif n_channel==22:\n",
    "        batch_size=4000\n",
    "    \n",
    "    replace_target_iter=1\n",
    "    total_episode= batch_size*replace_target_iter*140\n",
    "    epsilon_update_period= batch_size*replace_target_iter*20\n",
    "    e_greedy= [0.3, 0.9, 1]\n",
    "    learning_rate=0.01\n",
    "    \n",
    "    \n",
    "    \n",
    "    flag_QLearning= True\n",
    "    \n",
    "    \n",
    "    \n",
    "                \n",
    "    '''Compare with Q-Learning'''   \n",
    "    if flag_QLearning:\n",
    "        # For fair comparision, initialize the DSA environment with same properties\n",
    "        env= copy.deepcopy(env_copy)\n",
    "        \n",
    "        # Initialize the Q-table for each SU\n",
    "        QL_list=[]\n",
    "        epsilon_index= np.zeros(n_su, dtype= int)\n",
    "        for k in range(n_su):\n",
    "            QL_tmp= QLearningTable(actions= list(range(env.n_actions)),learning_rate= learning_rate, \n",
    "                                   reward_decay=0.9,\n",
    "                                  e_greedy=e_greedy[0])\n",
    "            QL_list.append(QL_tmp)\n",
    "        \n",
    "        # SU sense the environment and get the sensing result\n",
    "        observation= env.sense()\n",
    "        \n",
    "        # Initialize the state and state_\n",
    "        state=[ [] for i in range(n_su)]\n",
    "        state_= [[] for i in range(n_su)]\n",
    "        for k in range(n_su):\n",
    "            state[k]= observation[k,: ]\n",
    "            \n",
    "        #Initialize some record values\n",
    "        reward_sum= np.zeros(n_su)\n",
    "        overall_reward_3= []\n",
    "        success_hist_3= []\n",
    "        fail_PU_hist_3=[]\n",
    "        fail_collision_hist_3=[]\n",
    "        success_sum=0\n",
    "        fail_PU_sum=0\n",
    "        fail_collision_sum=0\n",
    "        \n",
    "        \n",
    "        \n",
    "        action= np.zeros(n_su).astype(np.int32)\n",
    "        for step in range(total_episode):\n",
    "            # SU choose action based on observation\n",
    "            for k in range(n_su):\n",
    "                action[k]= QL_list[k].choose_action(str(state[k]))\n",
    "            # Update the environment based on independent Markov chain\n",
    "            \n",
    "            env.render()\n",
    "            env.render_SINR()\n",
    "            \n",
    "            # SU take action and obtain reward\n",
    "            reward= env.access(action)\n",
    "            \n",
    "            # Record reward, interference, collision\n",
    "            reward_sum= reward_sum+reward\n",
    "            # Record the number of successful transmission\n",
    "            success_sum= success_sum+env.success\n",
    "            # Record the number of collsion with PU\n",
    "            fail_PU_sum= fail_PU_sum+env.fail_PU\n",
    "            # Record the number of collision with SU\n",
    "            fail_collision_sum= fail_collision_sum+env.fail_collision\n",
    "            \n",
    "            \n",
    "            # SU sense the environment and get sensing result(contains sensing error)\n",
    "            observation_= env.sense()\n",
    "            \n",
    "            # Store one episode(s, a, r, s_)\n",
    "            for k in range(n_su):\n",
    "                state[k]= observation[k, :]\n",
    "                state_[k]= observation_[k, :]\n",
    "                \n",
    "            # Each SU learns their QL model\n",
    "            for k in range(n_su):\n",
    "                QL_list[k].learn(str(state[k]), action[k], reward[k], str(state_[k]))\n",
    "                \n",
    "            if ((step+1)%batch_size==0):\n",
    "                # Record reward, number of success/interference/collision\n",
    "                overall_reward_3.append(np.sum(reward_sum)/batch_size/n_su)\n",
    "                success_hist_3.append(success_sum/n_su)\n",
    "                fail_PU_hist_3.append(fail_PU_sum/n_su)\n",
    "                fail_collision_hist_3.append(fail_collision_sum/n_su)\n",
    "                \n",
    "                # After one batch, refresh the record\n",
    "                reward_sum= np.zeros(n_su)\n",
    "                success_sum=0\n",
    "                fail_PU_sum=0\n",
    "                fail_collsion_sum=0\n",
    "                \n",
    "                \n",
    "            # Update epsilon\n",
    "            if ((step+1)% epsilon_update_period)==0:\n",
    "                for k in range(n_su):\n",
    "                    epsilon_index[k]=min(len(e_greedy)-1, epsilon_index[k]+1)\n",
    "                print('Epsilon updated to %.1f'%QL_list[k].epsilon)\n",
    "                \n",
    "            # Print record after replacing DQN_target\n",
    "            if ((step+1)% (batch_size*replace_target_iter)==0):\n",
    "                print('Training time= %d; success=%d; fail_PU=%d; fail_collision=%d'%\n",
    "                     ((step+1), success_hist_3[-1], fail_PU_hist_3[-1], fail_collision_hist_3[-1]))\n",
    "                print('overall_reward_3=%.4f'%overall_reward_3[-1])\n",
    "                \n",
    "            # Swap observation\n",
    "            observation= observation_\n",
    "            \n",
    "            \n",
    "    \n",
    "    file_folder='.\\\\result\\\\channel_%d_su_%d_punish_-2'%(n_channel, n_su)\n",
    "    \n",
    "    np.save(file_folder+ '\\\\PU_TX_x', env.PU_TX_x)\n",
    "    np.save(file_folder+ '\\\\PU_TX_y', env.PU_TX_y)\n",
    "    np.save(file_folder+ '\\\\PU_RX_x', env.PU_RX_x)\n",
    "    np.save(file_folder+ '\\\\PU_RX_y', env.PU_RX_y)\n",
    "    np.save(file_folder+ '\\\\SU_TX_x', env.SU_TX_x)\n",
    "    np.save(file_folder+ '\\\\SU_TX_y', env.SU_TX_y)\n",
    "    np.save(file_folder+ '\\\\SU_RX_x', env.SU_RX_x)\n",
    "    np.save(file_folder+ '\\\\SU_RX_y', env.SU_RX_y)\n",
    "    \n",
    "    \n",
    "    if flag_QLearning:\n",
    "        np.save(file_folder+'\\\\success_hist_3', success_hist_3)\n",
    "        np.save(file_folder+'\\\\fail_PU_hist_3', fail_PU_hist_3)\n",
    "        np.save(file_folder+'\\\\fail_collision_hist_3', fail_collision_hist_3)\n",
    "        np.save(file_folder+'\\\\overall_reward_3', overall_reward_3)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-bracket",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
